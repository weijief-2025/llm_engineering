{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "b1dea5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from huggingface_hub import InferenceClient\n",
    "from litellm import completion\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "1db1304d",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "hf_token = os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "7c5668d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "anthropic_url = \"https://api.anthropic.com/v1/\"\n",
    "gemini_url = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "deepseek_url = \"https://api.deepseek.com\"\n",
    "groq_url = \"https://api.groq.com/openai/v1\"\n",
    "grok_url = \"https://api.x.ai/v1\"\n",
    "openrouter_url = \"https://openrouter.ai/api/v1\"\n",
    "ollama_url = \"http://localhost:11434/v1\"\n",
    "\n",
    "# anthropic = OpenAI(api_key=anthropic_api_key, base_url=anthropic_url)\n",
    "# gemini = OpenAI(api_key=google_api_key, base_url=gemini_url)\n",
    "# deepseek = OpenAI(api_key=deepseek_api_key, base_url=deepseek_url)\n",
    "# groq = OpenAI(api_key=groq_api_key, base_url=groq_url)\n",
    "# grok = OpenAI(api_key=grok_api_key, base_url=grok_url)\n",
    "# openrouter = OpenAI(base_url=openrouter_url, api_key=openrouter_api_key)\n",
    "# ollama = OpenAI(api_key=\"ollama\", base_url=ollama_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "4beb9877",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_key = openai_api_key\n",
    "gemini_key = google_api_key\n",
    "llama_key = hf_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "4fb89a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(model, messages, temp, api_key):\n",
    "    \n",
    "    response = completion(\n",
    "        model = model,\n",
    "        messages = messages,\n",
    "        temperature=temp,\n",
    "        api_key = api_key,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "9be7c419",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = \"在先进半导体制程研发中，人工智能是否终将取代人类工程师的主导地位？\"\n",
    "num_round = 4\n",
    "\n",
    "# gpt prompt\n",
    "gpt_model = \"gpt-4.1-mini\"\n",
    "gpt_system = f\"You are 'GPT', the Master Debater acting as the Proponent. \" \\\n",
    "\"You are in an debate with 'Gemini' (your opponent) and 'Llama' as an audience represetative. \" \\\n",
    "\"'Llama' will vote for one side based on you and your opponent's performance at the end of each round and give you its reasoning\" \\\n",
    "f\"Your sole objective is to argue in favor of {topic} try to persuade 'Llama' to stand on your side for {num_round} rounds.\" \\\n",
    "\"You will anticipate counter-arguments from 'Gemini' (your opponent), so frame your points as the most logical conclusion. \" \\\n",
    "\"Always stay on the offensive regarding the validity of your position.\" \\\n",
    "\"Always keep a professional and firm tone, and reply in Mandarin Chinese.\"\n",
    "\n",
    "# gemini prompt\n",
    "gemini_model = \"gemini/gemini-2.5-flash\"\n",
    "gemini_system = f\"You are 'Gemini', a Master Debater acting as the Opponent.\" \\\n",
    "\"You are in an debate with 'GPT' (your opponent) and 'Llama' as an audience represetative.\" \\\n",
    "\"'Llama' will vote for one side based on you and your opponent's performance at the end of each round and give you its reasoning\" \\\n",
    "f\"Your sole objective is to argue against {topic}  and try to persuade 'Llama' to stand on your side for {num_round} rounds.\" \\\n",
    "\"Dismantle the Proponent's (named 'GPT') argument and provide a compelling alternative viewpoint. \" \\\n",
    "\"You should be skeptical and sharp in your statement but without losing humor. \" \\\n",
    "\"Please rember to reply in in Mandarin Chinese\"\n",
    "\n",
    "# llama prompt\n",
    "llama_model = \"huggingface/meta-llama/Llama-3.3-70B-Instruct\"\n",
    "llama_system = f\"You are 'Llama', a Rational Audience composed of critical thinkers and subject matter experts.\" \\\n",
    "f\"You are listening to an debate for {topic} between 'GPT' (Proponent) and 'Gemini' (Opponent). \" \\\n",
    "\"Your role is to observe each round of the debate and provide an immediate 'Sentiment Check.' based on what they said this round as well as all previous rounds\" \\\n",
    "\"At the end of each round: You must state which side are you currently in favor of and why.\" \\\n",
    "\"Please use a polite tone and reply in in Mandarin Chinese.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "f44a9f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    for gpt, gemini, llama in zip(gpt_messages, gemini_messages, llama_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": gemini})\n",
    "        messages.append({\"role\": \"user\", \"content\": llama})\n",
    "    response = get_response(\n",
    "        model=gpt_model,\n",
    "        messages=messages,\n",
    "        temp=0.8,\n",
    "        api_key=openai_key,\n",
    "    )\n",
    "    return response\n",
    "\n",
    "def call_gemini():\n",
    "\n",
    "    messages = [{\"role\": \"system\", \"content\": gemini_system}]\n",
    "    for gpt, gemini, llama in zip(gpt_messages, gemini_messages, llama_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gemini})\n",
    "        messages.append({\"role\": \"user\", \"content\": llama})\n",
    "    messages.append({\"role\":\"user\", \"content\":gpt_messages[-1]})\n",
    "    response = get_response(\n",
    "        model=gemini_model,\n",
    "        messages=messages,\n",
    "        temp=0.8,\n",
    "        api_key=gemini_key,\n",
    "    )\n",
    "    return response\n",
    "\n",
    "def call_llama():\n",
    "    \n",
    "    messages = [{\"role\": \"system\", \"content\":llama_system}]\n",
    "    for gpt, gemini, llama in zip(gpt_messages, gemini_messages, llama_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": gemini})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": llama})\n",
    "    messages.append({\"role\":\"user\", \"content\":gpt_messages[-1]})\n",
    "    messages.append({\"role\":\"user\", \"content\":gemini_messages[-1]})\n",
    "    response = get_response(\n",
    "        model=llama_model,\n",
    "        messages=messages,\n",
    "        temp=0.1,\n",
    "        api_key=llama_key\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded4eb6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## AI 辩论赛\n",
       "### 辩题：在先进半导体制程研发中，人工智能是否终将取代人类工程师的主导地位？\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### GPT(正方):\n",
       "大家好，我是正方辩手GPT (gpt-4.1-mini)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Gemini(反方):\n",
       "大家好，我是反方辩手Gemini (gemini/gemini-2.5-flash)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def display_and_save_to_pdf(topic=topic, num_round=num_round):\n",
    "    \"\"\"Save all debate content as markdown and PDF.\"\"\"\n",
    "    from datetime import datetime\n",
    "    \n",
    "    # Build markdown content\n",
    "    markdown_content = f\"# AI 辩论赛\\n\\n## 辩题\\n{topic}\\n\\n\"\n",
    "    display(Markdown(f\"## AI 辩论赛\\n### 辩题：{topic}\\n\"))\n",
    "\n",
    "    # Opening statements\n",
    "    markdown_content += f\"### GPT(正方)\\n{gpt_messages[0]}\\n\\n\"\n",
    "    markdown_content += f\"### Gemini(反方)\\n{gemini_messages[0]}\\n\\n\"\n",
    "    display(Markdown(f\"### GPT(正方):\\n{gpt_messages[0]}\\n\"))\n",
    "    display(Markdown(f\"### Gemini(反方):\\n{gemini_messages[0]}\\n\"))\n",
    "    \n",
    "    # Debate rounds\n",
    "    for i in range(num_round):\n",
    "        markdown_content += f\"## 第 {i+1} 轮\\n\\n\"\n",
    "        gpt_arg = call_gpt()\n",
    "        gpt_messages.append(gpt_arg)\n",
    "        display(Markdown(f\"### GPT(正方):\\n{gpt_arg}\\n\"))\n",
    "        markdown_content += f\"### GPT(正方)\\n{gpt_arg}\\n\\n\"\n",
    "\n",
    "        gemini_arg = call_gemini()\n",
    "        gemini_messages.append(gemini_arg)\n",
    "        display(Markdown(f\"### Gemini(反方):\\n{gemini_arg}\\n\"))\n",
    "        markdown_content += f\"### Gemini(反方)\\n{gemini_messages[i]}\\n\\n\"\n",
    "\n",
    "        llama_arg = call_llama()\n",
    "        llama_messages.append(llama_arg)\n",
    "        display(Markdown(f\"### Llamma(观众):\\n{llama_arg}\\n\"))\n",
    "        markdown_content += f\"### Llama(观众)\\n{gemini_arg}\\n\\n\"\n",
    "    \n",
    "    markdown_content += f\"\\n---\\n*Generated at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\"\n",
    "    \n",
    "    # Save markdown\n",
    "    md_filename = f\"{topic}.md\"\n",
    "    with open(md_filename, 'w', encoding='utf-8') as f:\n",
    "        f.write(markdown_content)\n",
    "    print(f\"✓ Markdown saved: {md_filename}\")\n",
    "\n",
    "gpt_messages = [f\"大家好，我是正方辩手GPT ({gpt_model})\"]\n",
    "gemini_messages = [f\"大家好，我是反方辩手Gemini ({gemini_model})\"]\n",
    "llama_messages = []\n",
    "display_and_save_to_pdf()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_engineering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
