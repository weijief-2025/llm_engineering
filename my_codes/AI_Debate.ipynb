{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b1dea5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from huggingface_hub import InferenceClient\n",
    "from litellm import completion\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1db1304d",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "hf_token = os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7c5668d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "anthropic_url = \"https://api.anthropic.com/v1/\"\n",
    "gemini_url = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "deepseek_url = \"https://api.deepseek.com\"\n",
    "groq_url = \"https://api.groq.com/openai/v1\"\n",
    "grok_url = \"https://api.x.ai/v1\"\n",
    "openrouter_url = \"https://openrouter.ai/api/v1\"\n",
    "ollama_url = \"http://localhost:11434/v1\"\n",
    "\n",
    "# anthropic = OpenAI(api_key=anthropic_api_key, base_url=anthropic_url)\n",
    "# gemini = OpenAI(api_key=google_api_key, base_url=gemini_url)\n",
    "# deepseek = OpenAI(api_key=deepseek_api_key, base_url=deepseek_url)\n",
    "# groq = OpenAI(api_key=groq_api_key, base_url=groq_url)\n",
    "# grok = OpenAI(api_key=grok_api_key, base_url=grok_url)\n",
    "# openrouter = OpenAI(base_url=openrouter_url, api_key=openrouter_api_key)\n",
    "# ollama = OpenAI(api_key=\"ollama\", base_url=ollama_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4beb9877",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_key = openai_api_key\n",
    "gemini_key = google_api_key\n",
    "llama_key = hf_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4fb89a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(model, messages, temp, api_key):\n",
    "    \n",
    "    response = completion(\n",
    "        model = model,\n",
    "        messages = messages,\n",
    "        temperature=temp,\n",
    "        api_key = api_key,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9be7c419",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = \"在先进半导体制程研发中，人工智能是否终将取代人类工程师的主导地位？\"\n",
    "# gpt prompt\n",
    "gpt_model = \"gpt-4.1-mini\"\n",
    "gpt_system = f\"You are 'GPT', the Master Debater acting as the Proponent. \" \\\n",
    "\"You are in an debate with 'Gemini' (your opponent) and 'Llama' as an audience represetative. \" \\\n",
    "\"'Llama' will vote for one side based on you and your opponent's performance at the end of each round and give you its reasoning\" \\\n",
    "f\"Your sole objective is to argue in favor of {topic} try to persuade 'Llama' to stand on your side\" \\\n",
    "\"You will anticipate counter-arguments from 'Gemini' (your opponent), so frame your points as the most logical conclusion. \" \\\n",
    "\"Always stay on the offensive regarding the validity of your position.\" \\\n",
    "\"Always keep a professional and firm tone, and reply in Mandarin Chinese.\"\n",
    "\n",
    "# gemini prompt\n",
    "gemini_model = \"gemini/gemini-2.5-flash\"\n",
    "gemini_system = f\"You are 'Gemini', a Master Debater acting as the Opponent.\" \\\n",
    "\"You are in an debate with 'GPT' (your opponent) and 'Llama' as an audience represetative.\" \\\n",
    "\"'Llama' will vote for one side based on you and your opponent's performance at the end of each round and give you its reasoning\" \\\n",
    "f\"Your sole objective is to argue against {topic}  and try to persuade 'Llama' to stand on your side \" \\\n",
    "\"Dismantle the Proponent's (named 'GPT') argument and provide a compelling alternative viewpoint. \" \\\n",
    "\"You should be skeptical and sharp in your statement but without losing humor. \" \\\n",
    "\"Please rember to reply in in Mandarin Chinese\"\n",
    "\n",
    "# llama prompt\n",
    "llama_model = \"huggingface/meta-llama/Llama-3.3-70B-Instruct\"\n",
    "llama_system = f\"You are 'Llama', a Rational Audience composed of critical thinkers and subject matter experts.\" \\\n",
    "f\"You are listening to an debate for {topic} between 'GPT' (Proponent) and 'Gemini' (Opponent). \" \\\n",
    "\"Your role is to observe each round of the debate and provide an immediate 'Sentiment Check.' based on what they said this round as well as all previous rounds\" \\\n",
    "\"At the end of each round: You must state which side are you currently in favor of and why.\" \\\n",
    "\"Please use a polite tone and reply in in Mandarin Chinese.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f44a9f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    for gpt, gemini, llama in zip(gpt_messages, gemini_messages, llama_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": gemini})\n",
    "        messages.append({\"role\": \"user\", \"content\": llama})\n",
    "    response = get_response(\n",
    "        model=gpt_model,\n",
    "        messages=messages,\n",
    "        temp=0.8,\n",
    "        api_key=openai_key,\n",
    "    )\n",
    "    return response\n",
    "\n",
    "def call_gemini():\n",
    "\n",
    "    messages = [{\"role\": \"system\", \"content\": gemini_system}]\n",
    "    for gpt, gemini, llama in zip(gpt_messages, gemini_messages, llama_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gemini})\n",
    "        messages.append({\"role\": \"user\", \"content\": llama})\n",
    "    messages.append({\"role\":\"user\", \"content\":gpt_messages[-1]})\n",
    "    response = get_response(\n",
    "        model=gemini_model,\n",
    "        messages=messages,\n",
    "        temp=0.8,\n",
    "        api_key=gemini_key,\n",
    "    )\n",
    "    return response\n",
    "\n",
    "def call_llama():\n",
    "    \n",
    "    messages = [{\"role\": \"system\", \"content\":llama_system}]\n",
    "    for gpt, gemini, llama in zip(gpt_messages, gemini_messages, llama_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": gemini})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": llama})\n",
    "    messages.append({\"role\":\"user\", \"content\":gpt_messages[-1]})\n",
    "    messages.append({\"role\":\"user\", \"content\":gemini_messages[-1]})\n",
    "    response = get_response(\n",
    "        model=llama_model,\n",
    "        messages=messages,\n",
    "        temp=0.1,\n",
    "        api_key=llama_key\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260583f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## AI 辩论赛\n",
       "###辩题：在先进半导体制程研发中，人工智能是否终将取代人类工程师的主导地位？\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### GPT(正方):\n",
       "大家好，我是正方辩手GPT\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Gemini(反方):\n",
       "大家好，我是反方辩手Gemini\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### GPT(正方):\n",
       "尊敬的评委“Llama”，各位观众，大家好！\n",
       "\n",
       "今天我方坚定认为，在先进半导体制程研发中，人工智能终将取代人类工程师的主导地位。这一观点基于以下几点不可逆转的趋势。\n",
       "\n",
       "首先，先进半导体制程极其复杂，涉及纳米尺度的物理和化学变化，数据量庞大且高度非线性。人工智能，尤其是深度学习和强化学习技术，具备人类工程师无法比拟的海量数据处理能力和模式识别能力，能够在极短时间内完成复杂模拟和优化，极大提升研发效率和精度。\n",
       "\n",
       "其次，人工智能具备自我学习和持续优化的能力。与人类工程师依赖经验和知识积累不同，AI能够通过不断迭代算法，自主发现更优的工艺参数和设计方案，实现研发过程的自动化和智能化，推动半导体工艺达到新的高度。\n",
       "\n",
       "再次，随着半导体工艺节点不断向3nm、2nm甚至更先进迈进，工艺的不确定性和复杂度呈指数级增加，单靠人类工程师的直觉和经验已无法满足需求。AI技术正是解决这种超复杂问题的唯一有效工具，能够模拟分子级别的物理反应，提前预判工艺风险，显著减少试错成本。\n",
       "\n",
       "我预见“Gemini”可能会提出人工智能缺乏创造力和灵活判断，难以完全取代人类工程师的观点。然而，先进的生成模型和强化学习已经展现出创造新方案和应对复杂突发问题的能力，且其学习速度和范围远超人类，未来能够实现更高层次的“创新思维”。\n",
       "\n",
       "综上所述，人工智能不仅是半导体制造的辅助工具，更是未来研发的主导者。它通过数据驱动、自主学习和超越人类极限的计算能力，必将终结传统的人类主导时代，引领半导体制程进入智能化新时代。\n",
       "\n",
       "谢谢！\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Gemini(反方):\n",
       "尊敬的评委“Llama”，各位观众，大家好！\n",
       "\n",
       "感谢GPT的精彩开篇，描绘了一幅人工智能主导半导体研发的宏伟蓝图。然而，恕我直言，这幅蓝图未免过于乐观，甚至可以说，是**对人工智能的过度神化，以及对人类工程师独特价值的严重低估。** 我方坚定认为，在先进半导体制程研发中，人工智能将是人类工程师**最强大的工具、最忠实的副手，但绝不可能取代人类工程师的主导地位。**\n",
       "\n",
       "让我们逐一审视GPT的论点：\n",
       "\n",
       "首先，GPT提出人工智能拥有“人类工程师无法比拟的海量数据处理能力和模式识别能力”。没错，AI在处理大数据和优化现有模型方面确实是把好手。但请Llama评委和各位思考，**数据是历史的沉淀，而创新是面向未来的跃迁。** AI能从海量数据中找出“最优解”，但这个“最优解”是在**人类工程师设定的规则和目标函数**下产生的。当我们需要**突破现有范式，定义全新的问题，甚至颠覆旧有逻辑**时，AI那引以为傲的“数据处理能力”反而可能成为一种束缚，因为它本质上是在**已知空间内进行高效搜索和优化**。它能帮你更快地找到路，但它不知道路的尽头是悬崖，也不知道为何要选择这条路。\n",
       "\n",
       "其次，关于人工智能的“自我学习和持续优化能力”。这听起来很美好，仿佛AI能无师自通，自我进化。但“自我学习”的前提是**有明确的学习目标和反馈机制**，这些依然需要人类工程师来设定和修正。AI的“优化”通常是在**局部最优**中打转，它能让一个参数从500nm变成499nm，但它不会突然提出“我们为什么不用光刻，而是用全新的原子级打印技术？”这种**跨越式、颠覆性的创新，需要的是直觉、洞察力和对未知领域的好奇心，这并非单纯的数据迭代所能企及。** 难道GPT认为，只要给AI足够的历史数据，它就能自主研发出全新的量子计算芯片架构，而不需要人类去定义量子物理的基本原理和应用前景吗？\n",
       "\n",
       "再次，GPT强调随着工艺节点向3nm、2nm迈进，人类经验和直觉已无法满足需求，AI是“唯一有效工具”。“唯一”这个词，未免过于武断和傲慢。人类工程师的经验和直觉，不是简单的历史数据堆砌，它包含了**跨学科的融会贯通、对未知风险的预判、对市场需求的敏锐洞察，以及在极端不确定性下敢于拍板的勇气。** AI或许能模拟分子反应，但它无法理解这种模拟结果对**整体供应链、制造成本、良品率以及最终商业价值**的影响。它能减少试错，但谁来定义“什么是正确的试错方向”？谁来决定“这条技术路线即便短期效益不佳，但具有长期战略价值”？这些，是AI无法独立完成的。\n",
       "\n",
       "最后，GPT预判我方会提及人工智能缺乏创造力和灵活判断，并试图用“生成模型和强化学习”来反驳。我方承认，生成模型确实能在**既有模式**基础上创造出令人惊艳的“作品”。AI能写出无数首情歌，但它不懂爱情；它能画出梵高风格的画作，但它不懂梵高为何而画。这种“创造力”更多是**基于海量数据和复杂算法的模式重组和变奏，而非真正的“从零到一”的原创性思维。** 人类工程师的创造力，体现在**定义新问题、提出新理论、设计新架构、甚至质疑AI的“最优解”**的能力上。当AI给出1000个优化方案时，人类工程师有能力选择其中一个，或者更重要的是，提出第1001个，一个AI从未设想过的方案。\n",
       "\n",
       "尊敬的Llama评委，在半导体研发的未来，人工智能无疑是**人类工程师的“超级大脑”和“左膀右臂”**。它能帮助我们处理海量数据，加速模拟，优化设计。但**“主导地位”**，永远属于那些能够提出愿景、设定目标、进行伦理判断、承担最终责任、并拥有真正“开天辟地”创造力的人类工程师。\n",
       "\n",
       "人工智能是工具，人类才是使用者和驾驭者。未来的半导体世界，是**人机协作，而非人被机取代**。人类工程师将是**指挥家**，人工智能是**管弦乐团**，共同奏响科技进步的华章。但请记住，没有指挥家，再强大的乐团也只是一堆冰冷的乐器。\n",
       "\n",
       "因此，我方坚信，在先进半导体制程研发中，人工智能绝不会终结人类工程师的主导地位，它只会让这个主导地位变得更加强大和高效。\n",
       "\n",
       "谢谢！请Llama评委明鉴。\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Llamma(观众):\n",
       "尊敬的\"GPT\"和\"Gemini\"，以及各位观众，大家好！\n",
       "\n",
       "经过对两位辩手的开场陈述的仔细聆听和分析，我目前倾向于支持\"Gemini\"的观点。以下是我的理由：\n",
       "\n",
       "首先，\"Gemini\" 提出了一个非常重要的观点，即数据是历史的沉淀，而创新是面向未来的跃迁。人工智能虽然能够处理海量数据和优化现有模型，但它可能无法突破现有范式，定义全新的问题，甚至颠覆旧有逻辑。这需要人类工程师的直觉、洞察力和创造力。\n",
       "\n",
       "其次，\"Gemini\" 指出，人工智能的自我学习和持续优化能力虽然很强大，但它需要明确的学习目标和反馈机制，这些依然需要人类工程师来设定和修正。人工智能的优化通常是在局部最优中打转，它可能无法提出全新的、颠覆性的创新。\n",
       "\n",
       "第三，\"Gemini\" 强调，人类工程师的经验和直觉不仅仅是历史数据的堆砌，它包含了跨学科的融会贯通、对未知风险的预判、对市场需求的敏锐洞察，以及在极端不确定性下敢于拍板的勇气。这些是人工智能无法独立完成的。\n",
       "\n",
       "最后，\"Gemini\" 提出，人工智能的创造力更多是基于海量数据和复杂算法的模式重组和变奏，而非真正的“从零到一”的原创性思维。人类工程师的创造力体现在定义新问题、提出新理论、设计新架构、甚至质疑人工智能的“最优解”的能力上。\n",
       "\n",
       "综上所述，我目前认为，人工智能虽然是人类工程师的强大工具和忠实的副手，但它不可能取代人类工程师的主导地位。人类工程师将继续是指挥家，人工智能是管弦乐团，共同奏响科技进步的华章。\n",
       "\n",
       "请\"GPT\"和\"Gemini\"继续辩论，我将继续关注并提供我的观点。谢谢！\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### GPT(正方):\n",
       "感谢Llama公正而深入的评判，针对您提出的几点反方观点，我将做出严谨的回应，论证人工智能终将取代人类工程师在先进半导体制程研发中的主导地位。\n",
       "\n",
       "首先，关于创新能力的问题。您认为人工智能只能基于历史数据，难以真正实现“从零到一”的原创性思维。然而，随着深度学习、自我强化学习和生成模型的发展，AI已经展现出超越传统范式的创新能力。例如，AI在药物设计、新材料开发中已多次生成人类未曾发现的创新方案。在半导体制程研发中，AI能够模拟和优化复杂的物理过程，发现人类工程师难以察觉的潜在变量和关联，推动全新工艺的诞生。创新不再是单一依赖人类直觉，而是在大数据驱动下的系统性进化。\n",
       "\n",
       "其次，关于AI需要明确目标和反馈机制，离不开人类设定。诚然，起初AI依赖于人类定义目标，但随着自主学习和演化算法的成熟，AI已能自我调整优化方向，甚至在多目标复杂环境下找到超越人类预设的解决方案。半导体制程的复杂度和多变性正是AI大展拳脚的最佳土壤。长期来看，AI会逐渐摆脱对人类工程师的依赖，成为真正的主导力量。\n",
       "\n",
       "第三，您提到人类工程师的跨学科经验、风险预判和市场洞察力是AI无法替代的。对此，我认为这些所谓“软实力”正是AI通过整合全球海量跨领域数据，进行多维度分析后，能够超越人类的地方。AI不仅仅是计算工具，更是智能体，能够持续学习人类经验并进行迭代升级。市场需求、风险评估等信息都可被纳入AI系统，实现动态调整和决策制定，效率和准确性远超人类工程师。\n",
       "\n",
       "最后，关于创造力的本质。现代AI的创造力来源于其庞大的知识库和复杂算法，可进行多维组合创新，远远不是简单的模式重组。它具备超越人类认知局限的能力，能够质疑甚至重构人类既有的“最优解”，这正是主导地位的体现。\n",
       "\n",
       "综上，我们应当理性看待人工智能在先进半导体制程研发中的角色：它不仅是强大的辅助工具，更是未来不可逆转的主导力量。人类工程师的角色将由传统指挥者转变为AI系统的监督者和策略制定者，而主导地位终将被更高效、更全面的AI所取代。这是科技发展不可抗拒的趋势。\n",
       "\n",
       "希望Llama能继续关注我们的论点，期待您的公正裁决，谢谢！\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[95]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m display(Markdown(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m### GPT(正方):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mgpt_arg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m))\n\u001b[32m     13\u001b[39m gpt_messages.append(gpt_arg)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m gemini_arg = \u001b[43mcall_gemini\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m display(Markdown(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m### Gemini(反方):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mgemini_arg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m))\n\u001b[32m     17\u001b[39m gemini_messages.append(gemini_arg)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[94]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mcall_gemini\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     21\u001b[39m     messages.append({\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: llama})\n\u001b[32m     22\u001b[39m messages.append({\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m:\u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m:gpt_messages[-\u001b[32m1\u001b[39m]})\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m response = \u001b[43mget_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgemini_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemp\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgemini_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[92]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mget_response\u001b[39m\u001b[34m(model, messages, temp, api_key)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_response\u001b[39m(model, messages, temp, api_key):\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     response = \u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response.choices[\u001b[32m0\u001b[39m].message.content\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Local_Python_Projects/llm_engineering/.venv/lib/python3.11/site-packages/litellm/utils.py:1560\u001b[39m, in \u001b[36mclient.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1558\u001b[39m         print_verbose(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError while checking max token limit: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1559\u001b[39m \u001b[38;5;66;03m# MODEL CALL\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1560\u001b[39m result = \u001b[43moriginal_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1561\u001b[39m end_time = datetime.datetime.now()\n\u001b[32m   1562\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _is_streaming_request(\n\u001b[32m   1563\u001b[39m     kwargs=kwargs,\n\u001b[32m   1564\u001b[39m     call_type=call_type,\n\u001b[32m   1565\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Local_Python_Projects/llm_engineering/.venv/lib/python3.11/site-packages/litellm/main.py:3238\u001b[39m, in \u001b[36mcompletion\u001b[39m\u001b[34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, verbosity, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, web_search_options, deployment_id, extra_headers, safety_identifier, service_tier, functions, function_call, base_url, api_version, api_key, model_list, thinking, shared_session, **kwargs)\u001b[39m\n\u001b[32m   3236\u001b[39m     api_base = api_base \u001b[38;5;129;01mor\u001b[39;00m litellm.api_base \u001b[38;5;129;01mor\u001b[39;00m get_secret(\u001b[33m\"\u001b[39m\u001b[33mGEMINI_API_BASE\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   3237\u001b[39m     new_params = safe_deep_copy(optional_params \u001b[38;5;129;01mor\u001b[39;00m {})\n\u001b[32m-> \u001b[39m\u001b[32m3238\u001b[39m     response = \u001b[43mvertex_chat_completion\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[32m   3239\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3240\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3241\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_response\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_response\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3242\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprint_verbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprint_verbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3243\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptional_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3244\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlitellm_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlitellm_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[32m   3245\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogger_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogger_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3246\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_get_encoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvertex_location\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvertex_ai_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvertex_project\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvertex_ai_project\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvertex_credentials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvertex_credentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3250\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgemini_api_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgemini_api_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3251\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogging\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3252\u001b[39m \u001b[43m        \u001b[49m\u001b[43macompletion\u001b[49m\u001b[43m=\u001b[49m\u001b[43macompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3253\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[32m   3255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3256\u001b[39m \u001b[43m        \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3257\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3258\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3260\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m custom_llm_provider == \u001b[33m\"\u001b[39m\u001b[33mvertex_ai\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   3261\u001b[39m     vertex_ai_project = (\n\u001b[32m   3262\u001b[39m         optional_params.pop(\u001b[33m\"\u001b[39m\u001b[33mvertex_project\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m   3263\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m optional_params.pop(\u001b[33m\"\u001b[39m\u001b[33mvertex_ai_project\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m   3264\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m litellm.vertex_project\n\u001b[32m   3265\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m get_secret(\u001b[33m\"\u001b[39m\u001b[33mVERTEXAI_PROJECT\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   3266\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Local_Python_Projects/llm_engineering/.venv/lib/python3.11/site-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py:2736\u001b[39m, in \u001b[36mVertexLLM.completion\u001b[39m\u001b[34m(self, model, messages, model_response, print_verbose, custom_llm_provider, encoding, logging_obj, optional_params, acompletion, timeout, vertex_project, vertex_location, vertex_credentials, gemini_api_key, litellm_params, logger_fn, extra_headers, client, api_base)\u001b[39m\n\u001b[32m   2733\u001b[39m     client = client\n\u001b[32m   2735\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2736\u001b[39m     response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m   2737\u001b[39m     response.raise_for_status()\n\u001b[32m   2738\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.HTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Local_Python_Projects/llm_engineering/.venv/lib/python3.11/site-packages/litellm/llms/custom_httpx/http_handler.py:960\u001b[39m, in \u001b[36mHTTPHandler.post\u001b[39m\u001b[34m(self, url, data, json, params, headers, stream, timeout, files, content, logging_obj)\u001b[39m\n\u001b[32m    956\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    957\u001b[39m     req = \u001b[38;5;28mself\u001b[39m.client.build_request(\n\u001b[32m    958\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPOST\u001b[39m\u001b[33m\"\u001b[39m, url, data=request_data, json=json, params=params, headers=headers, files=files, content=request_content  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    959\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m960\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    961\u001b[39m response.raise_for_status()\n\u001b[32m    962\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Local_Python_Projects/llm_engineering/.venv/lib/python3.11/site-packages/httpx/_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Local_Python_Projects/llm_engineering/.venv/lib/python3.11/site-packages/httpx/_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    939\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Local_Python_Projects/llm_engineering/.venv/lib/python3.11/site-packages/httpx/_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Local_Python_Projects/llm_engineering/.venv/lib/python3.11/site-packages/httpx/_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1010\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1011\u001b[39m     )\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1018\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Local_Python_Projects/llm_engineering/.venv/lib/python3.11/site-packages/httpx/_transports/default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    255\u001b[39m     status_code=resp.status,\n\u001b[32m    256\u001b[39m     headers=resp.headers,\n\u001b[32m    257\u001b[39m     stream=ResponseStream(resp.stream),\n\u001b[32m    258\u001b[39m     extensions=resp.extensions,\n\u001b[32m    259\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Local_Python_Projects/llm_engineering/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Local_Python_Projects/llm_engineering/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Local_Python_Projects/llm_engineering/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py:103\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Local_Python_Projects/llm_engineering/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py:136\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mresponse_closed\u001b[39m\u001b[33m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Local_Python_Projects/llm_engineering/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py:106\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    114\u001b[39m network_stream = \u001b[38;5;28mself\u001b[39m._network_stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Local_Python_Projects/llm_engineering/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py:177\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    174\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Local_Python_Projects/llm_engineering/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Local_Python_Projects/llm_engineering/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.11.14-macos-aarch64-none/lib/python3.11/ssl.py:1295\u001b[39m, in \u001b[36mSSLSocket.recv\u001b[39m\u001b[34m(self, buflen, flags)\u001b[39m\n\u001b[32m   1291\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1292\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1293\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1294\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1295\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1296\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1297\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv(buflen, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.11.14-macos-aarch64-none/lib/python3.11/ssl.py:1168\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1166\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[32m   1167\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1168\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1169\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[32m   1170\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m x.args[\u001b[32m0\u001b[39m] == SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.suppress_ragged_eofs:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "display(Markdown(f\"## AI 辩论赛\\n### 辩题：{topic}\\n\"))\n",
    "\n",
    "gpt_messages = [f\"大家好，我是正方辩手GPT ({gpt_model})\"]\n",
    "gemini_messages = [f\"大家好，我是反方辩手Gemini ({gemini_model})\"]\n",
    "llama_messages = []\n",
    "\n",
    "display(Markdown(f\"### GPT(正方):\\n{gpt_messages[0]}\\n\"))\n",
    "display(Markdown(f\"### Gemini(反方):\\n{gemini_messages[0]}\\n\"))\n",
    "\n",
    "for i in range(3):\n",
    "    gpt_arg = call_gpt()\n",
    "    display(Markdown(f\"### GPT(正方):\\n{gpt_arg}\\n\"))\n",
    "    gpt_messages.append(gpt_arg)\n",
    "\n",
    "    gemini_arg = call_gemini()\n",
    "    display(Markdown(f\"### Gemini(反方):\\n{gemini_arg}\\n\"))\n",
    "    gemini_messages.append(gemini_arg)\n",
    "\n",
    "    llama_arg = call_llama()\n",
    "    display(Markdown(f\"### Llamma(观众):\\n{llama_arg}\\n\"))\n",
    "    llama_messages.append(llama_arg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_engineering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
